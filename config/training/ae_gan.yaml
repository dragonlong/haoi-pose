# @package _global_
module: ae  # ['ae', 'vae', 'gan']
n_pts: 1024 # network output, hyper_para
# network
enc_filters: (64, 128, 128, 256)
latent_dim: 128
dec_features: (256, 256)
# treeGAN
DEGREE: [1,  2,   2,   2,   2,   2,   64]
G_FEAT: [96, 256, 256, 256, 128, 128, 128, 3]
D_FEAT: [3, 64,  128, 256, 256, 512]
support: 10
loop_non_linear: False
#
nocs_features: [128, 128, 3, 'sigmoid']
seg_features: [128, 128, 2, 'softmax']
confi_features: [128, 1, 'sigmoid']
mode_features: ['softmax']
enc_bn: True
dec_bn: False
z_dim: 64 # dimension for mode condition vector z
# GAN
zEnc_features: (256, 512)
G_features: (256, 512)
D_features: (256, 512)
pretrain_ae_path: None
pretrain_vae_path: None
nr_epochs: 300
lr: 0.0005       # initial learning rate
lr_decay: 0.9995 # step size for learning rate decay
beta1_gan: 0.5 # beta1 for Adam Optimizer when training gan
parallel: False
multimodal: True
# additional controlling
cont: False
ckpt: latest
vis: False
save_frequency: 1
save_step_frequency: 1000
val_frequency: 10
eval_frequency: 1000
vis_frequency: 1000
weight_kl_vae: 10.0   # weight factor for kl loss
weight_z_L1: 7.5      # weight factor for latent reconstruction loss
weight_partial_rec: 6 # weight factor for partial reconstruction loss
# testing
num_sample: 10
num_z: 5

TRAIN:
  train_batch: 2
  test_batch: 2
  workers: 8
